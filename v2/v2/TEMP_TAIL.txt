    tds = bet365_row.find_all("td")
    if len(tds) >= 11:
        odds_info["ah_home_cuota"] = tds[2].get("data-o", tds[2].text).strip()
        odds_info["ah_linea_raw"] = tds[3].get("data-o", tds[3].text).strip()
        odds_info["ah_away_cuota"] = tds[4].get("data-o", tds[4].text).strip()
        odds_info["goals_over_cuota"] = tds[8].get("data-o", tds[8].text).strip()
        odds_info["goals_linea_raw"] = tds[9].get("data-o", tds[9].text).strip()
        odds_info["goals_under_cuota"] = tds[10].get("data-o", tds[10].text).strip()
    return odds_info

def extract_standings_data_from_h2h_page_of(soup, team_name):
    data = {"name": team_name, "ranking": "N/A", "total_pj": "N/A", "total_v": "N/A",
            "total_e": "N/A", "total_d": "N/A", "total_gf": "N/A", "total_gc": "N/A",
            "specific_pj": "N/A", "specific_v": "N/A", "specific_e": "N/A",
            "specific_d": "N/A", "specific_gf": "N/A", "specific_gc": "N/A",
            "specific_type": "N/A"}
    if not soup or not team_name:
        return data
    standings_section = soup.find("div", id="porletP4")
    if not standings_section:
        return data
    team_table_soup = None
    is_home_table = False
    home_div = standings_section.find("div", class_="home-div")
    if home_div and team_name.lower() in home_div.get_text(strip=True).lower():
        team_table_soup = home_div.find("table", class_="team-table-home")
        is_home_table = True
        data["specific_type"] = "Est. como Local (en Liga)"
    else:
        guest_div = standings_section.find("div", class_="guest-div")
        if guest_div and team_name.lower() in guest_div.get_text(strip=True).lower():
            team_table_soup = guest_div.find("table", class_="team-table-guest")
            is_home_table = False
            data["specific_type"] = "Est. como Visitante (en Liga)"
    if not team_table_soup:
        return data
    header_link = team_table_soup.find("a")
    if header_link:
        full_text = header_link.get_text(separator=" ", strip=True)
        rank_match = re.search(r'\['.*?-(\d+)\]', full_text)
        if rank_match:
            data["ranking"] = rank_match.group(1)
    all_rows = team_table_soup.find_all("tr", align="center")
    is_ft_section = False
    for row in all_rows:
        header_cell = row.find("th")
        if header_cell:
            header_text = header_cell.get_text(strip=True)
            if "FT" in header_text:
                is_ft_section = True
            elif "HT" in header_text:
                is_ft_section = False
            continue
        if is_ft_section and len(cells := row.find_all("td")) >= 7:
            row_type_element = cells[0].find("span") or cells[0]
            row_type = row_type_element.get_text(strip=True)
            stats = [cell.get_text(strip=True) for cell in cells[1:7]]
            pj, v, e, d, gf, gc = stats
            if row_type == "Total":
                data.update({"total_pj": pj, "total_v": v, "total_e": e,
                            "total_d": d, "total_gf": gf, "total_gc": gc})
            specific_row_needed = "Home" if is_home_table else "Away"
            if row_type == specific_row_needed:
                data.update({"specific_pj": pj, "specific_v": v, "specific_e": e,
                            "specific_d": d, "specific_gf": gf, "specific_gc": gc})
    return data

def extract_over_under_stats_from_div_of(soup, team_type: str):
    default_stats = {"over_pct": 0, "under_pct": 0, "push_pct": 0, "total": 0}
    if not soup:
        return default_stats
    table_id = "table_v1" if team_type == 'home' else "table_v2"
    table = soup.find("table", id=table_id)
    if not table:
        return default_stats
    y_bar = table.find("ul", class_="y-bar")
    if not y_bar:
        return default_stats
    ou_group = None
    for group in y_bar.find_all("li", class_="group"):
        if "Over/Under Odds" in group.get_text():
            ou_group = group
            break
    if not ou_group:
        return default_stats
    try:
        total_text = ou_group.find("div", class_="tit").find("span").get_text(strip=True)
        total_match = re.search(r'\((\d+)\s*games\)', total_text)
        total = int(total_match.group(1)) if total_match else 0
        values = ou_group.find_all("span", class_="value")
        if len(values) == 3:
            over_pct_text = values[0].get_text(strip=True).replace('%', '')
            push_pct_text = values[1].get_text(strip=True).replace('%', '')
            under_pct_text = values[2].get_text(strip=True).replace('%', '')
            return {"over_pct": float(over_pct_text), "under_pct": float(under_pct_text), "push_pct": float(push_pct_text), "total": total}
    except (ValueError, TypeError, AttributeError):
        return default_stats
    return default_stats

def extract_h2h_data_of(soup, home_name, away_name, league_id=None):
    results = {'ah1': '-', 'res1': '?:?', 'res1_raw': '?-?', 'match1_id': None, 'ah6': '-', 'res6': '?:?', 'res6_raw': '?-?', 'match6_id': None, 'h2h_gen_home': "Local (H2H Gen)", 'h2h_gen_away': "Visitante (H2H Gen)"}
    if not soup or not home_name or not away_name or not (h2h_table := soup.find("table", id="table_v3")): return results
    all_matches = []
    for r in h2h_table.find_all("tr", id=re.compile(r"tr3_\d+")):
        if (d := get_match_details_from_row_of(r, score_class_selector='fscore_3', source_table_type='h2h')):
            if not league_id or (d.get('league_id_hist') and d.get('league_id_hist') == str(league_id)):
                all_matches.append(d)
    if not all_matches: return results
    all_matches.sort(key=lambda x: _parse_date_ddmmyyyy(x.get('date', '')), reverse=True)
    most_recent = all_matches[0]
    results.update({'ah6': most_recent.get('ahLine', '-'), 'res6': most_recent.get('score', '?:?'), 'res6_raw': most_recent.get('score_raw', '?-?'), 'match6_id': most_recent.get('matchIndex'), 'h2h_gen_home': most_recent.get('home'), 'h2h_gen_away': most_recent.get('away')})
    for d in all_matches:
        if d['home'].lower() == home_name.lower() and d['away'].lower() == away_name.lower():
            results.update({'ah1': d.get('ahLine', '-'), 'res1': d.get('score', '?:?'), 'res1_raw': d.get('score_raw', '?-?'), 'match1_id': d.get('matchIndex')})
            break
    return results

def extract_comparative_match_of(soup, table_id, main_team, opponent, league_id, is_home_table):
    if not opponent or opponent == "N/A" or not main_team or not (table := soup.find("table", id=table_id)): return None
    score_selector = 'fscore_1' if is_home_table else 'fscore_2'
    for row in table.find_all("tr", id=re.compile(rf"tr{table_id[-1]}_\d+")):
        if not (details := get_match_details_from_row_of(row, score_class_selector=score_selector, source_table_type='hist')): continue
        if league_id and details.get('league_id_hist') and details.get('league_id_hist') != str(league_id): continue
        h, a = details.get('home','').lower(), details.get('away','').lower()
        main, opp = main_team.lower(), opponent.lower()
        if (main == h and opp == a) or (main == a and opp == h):
            return {"score": details.get('score', '?:?'), "ah_line": details.get('ahLine', '-'), "localia": 'H' if main == h else 'A', "home_team": details.get('home'), "away_team": details.get('away'), "match_id": details.get('matchIndex')}
    return None

# --- FUNCIÓN PRINCIPAL DE EXTRACCIÓN ---

def obtener_datos_completos_partido(match_id: str):
    """
    Función principal que orquesta todo el scraping y análisis para un ID de partido.
    Devuelve un diccionario con todos los datos necesarios para la plantilla HTML.
    """
    if not match_id or not match_id.isdigit():
        return {"error": "ID de partido inválido."}

    # --- Inicialización de Selenium ---
    options = ChromeOptions()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36")
    options.add_argument('--blink-settings=imagesEnabled=false')
    driver = webdriver.Chrome(options=options)
    
    main_page_url = f"{BASE_URL_OF}/match/h2h-{match_id}"
    datos = {"match_id": match_id}

    try:
        # --- Carga y Parseo de la Página Principal ---
        driver.get(main_page_url)
        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.ID, "table_v1")))
        for select_id in ["hSelect_1", "hSelect_2", "hSelect_3"]:
            try:
                Select(WebDriverWait(driver, 3).until(EC.presence_of_element_located((By.ID, select_id)))).select_by_value("8")
                time.sleep(0.2)
            except TimeoutException:
                continue
        soup_completo = BeautifulSoup(driver.page_source, "lxml")

        # --- Extracción de Datos Primarios ---
        home_id, away_id, league_id, home_name, away_name, league_name = get_team_league_info_from_script_of(soup_completo)
        datos.update({"home_name": home_name, "away_name": away_name, "league_name": league_name})

        # --- Recopilación de todos los datos en paralelo (donde sea posible) ---
        with ThreadPoolExecutor(max_workers=8) as executor:
            # Tareas síncronas (dependen del soup_completo)
            future_home_standings = executor.submit(extract_standings_data_from_h2h_page_of, soup_completo, home_name)
            future_away_standings = executor.submit(extract_standings_data_from_h2h_page_of, soup_completo, away_name)
            future_home_ou = executor.submit(extract_over_under_stats_from_div_of, soup_completo, 'home')
            future_away_ou = executor.submit(extract_over_under_stats_from_div_of, soup_completo, 'away')
            future_main_odds = executor.submit(extract_bet365_initial_odds_of, soup_completo)
            future_h2h_data = executor.submit(extract_h2h_data_of, soup_completo, home_name, away_name, None)
            future_last_home = executor.submit(extract_last_match_in_league_of, soup_completo, "table_v1", home_name, league_id, True)
            future_last_away = executor.submit(extract_last_match_in_league_of, soup_completo, "table_v2", away_name, league_id, False)
            
            # Tarea H2H Col3 (requiere una nueva llamada de Selenium)
            key_id_a, rival_a_id, rival_a_name = get_rival_a_for_original_h2h_of(soup_completo, league_id)
            _, rival_b_id, rival_b_name = get_rival_b_for_original_h2h_of(soup_completo, league_id)
            future_h2h_col3 = executor.submit(get_h2h_details_for_original_logic_of, driver, key_id_a, rival_a_id, rival_b_id, rival_a_name, rival_b_name)
            
            # Obtener resultados
            datos["home_standings"] = future_home_standings.result()
            datos["away_standings"] = future_away_standings.result()
            datos["home_ou_stats"] = future_home_ou.result()
            datos["away_ou_stats"] = future_away_ou.result()
            main_match_odds_data = future_main_odds.result()
            h2h_data = future_h2h_data.result()
            last_home_match = future_last_home.result()
            last_away_match = future_last_away.result()
            details_h2h_col3 = future_h2h_col3.result()

            # --- Comparativas (dependen de los resultados anteriores) ---
            comp_L_vs_UV_A = extract_comparative_match_of(soup_completo, "table_v1", home_name, (last_away_match or {}).get('home_team'), league_id, True)
            comp_V_vs_UL_H = extract_comparative_match_of(soup_completo, "table_v2", away_name, (last_home_match or {}).get('away_team'), league_id, False)

            # --- Generar Análisis de Mercado ---
            datos["market_analysis_html"] = generar_analisis_completo_mercado(main_match_odds_data, h2h_data, home_name, away_name)

            # --- Estructurar datos para la plantilla ---
            datos["main_match_odds"] = {
                "ah_linea": format_ah_as_decimal_string_of(main_match_odds_data.get('ah_linea_raw', '?')),
                "goals_linea": format_ah_as_decimal_string_of(main_match_odds_data.get('goals_linea_raw', '?'))
            }
            
            # Recopilar todos los IDs de partidos históricos para obtener sus estadísticas de progresión
            match_ids_to_fetch_stats = {
                'last_home': (last_home_match or {}).get('match_id'),
                'last_away': (last_away_match or {}).get('match_id'),
                'h2h_col3': (details_h2h_col3 or {}).get('match_id'),
                'comp_L_vs_UV_A': (comp_L_vs_UV_A or {}).get('match_id'),
                'comp_V_vs_UL_H': (comp_V_vs_UL_H or {}).get('match_id'),
                'h2h_stadium': h2h_data.get('match1_id'),
                'h2h_general': h2h_data.get('match6_id')
            }
            
            # Obtener estadísticas de progresión en paralelo
            stats_futures = {key: executor.submit(get_match_progression_stats_data, match_id)
                             for key, match_id in match_ids_to_fetch_stats.items() if match_id}
                             
            stats_results = {key: future.result() for key, future in stats_futures.items()}

            # Empaquetar todo en el diccionario de datos final
            datos['last_home_match'] = {'details': last_home_match, 'stats': stats_results.get('last_home')}
            datos['last_away_match'] = {'details': last_away_match, 'stats': stats_results.get('last_away')}
            datos['h2h_col3'] = {'details': details_h2h_col3, 'stats': stats_results.get('h2h_col3')}
            datos['comp_L_vs_UV_A'] = {'details': comp_L_vs_UV_A, 'stats': stats_results.get('comp_L_vs_UV_A')}
            datos['comp_V_vs_UL_H'] = {'details': comp_V_vs_UL_H, 'stats': stats_results.get('comp_V_vs_UL_H')}
            datos['h2h_stadium'] = {'details': h2h_data, 'stats': stats_results.get('h2h_stadium')}
            datos['h2h_general'] = {'details': h2h_data, 'stats': stats_results.get('h2h_general')}

            # --- ANÁLISIS AVANZADO DE COMPARATIVAS INDIRECTAS ---
            datos['advanced_analysis_html'] = generar_analisis_comparativas_indirectas(
                datos.get('comp_L_vs_UV_A'),
                datos.get('comp_V_vs_UL_H'),
                datos.get('home_name'),
                datos.get('away_name')
            )
        
        return datos

    except Exception as e:
        print(f"ERROR CRÍTICO en el scraper: {e}")
        return {"error": f"Error durante el scraping: {e}"}
    finally:
        driver.quit()
